{"bucketlist":{"content":"\r\nThis is an ever-growing, ever-changing list of things I would like to accomplish as a software engineer, both in my professional and personal life.\r\n\r\n- [ ] Find an interesting open-source project and submit a PR within 24 hours \r\n- [ ] File a patent for software you wrote (at work or otherwise)\r\n- [x] Write a REST API\r\n- [x] Write a serverless function\r\n- [x] Containerize an application\r\n- [x] Make your own website\r\n- [ ] Work pager duty\r\n- [x] Participate in a hackathon\r\n- [x] Travel to attend a major software conference\r\n- [ ] Travel to speak or present at a major software conference\r\n- [x] Find a senior developer mentor\r\n- [ ] Mentor a junior developer\r\n- [ ] Submit code to these online package managers\r\n    - [x] PyPI\r\n    - [x] NuGet\r\n    - [ ] NPM\r\n- [ ] Have a stranger submit a meaningful PR to an open-source project you created\r\n- [ ] Pull an all-nighter coding on a personal project\r\n- [x] Write a piece of software just for you that you actually use\r\n- [ ] Reading list: \r\n    - [ ] [Clean Code](https://www.amazon.com/Clean-Code-Handbook-Software-Craftsmanship/dp/0132350882)\r\n    - [x] [The Night Watch](https://www.usenix.org/system/files/1311_05-08_mickens.pdf)\r\n    - [x] [Cracking the Coding Interview](https://www.amazon.com/Cracking-Coding-Interview-Programming-Questions/dp/0984782850/ref=dp_ob_title_bk)\r\n    - [ ] [Design Patterns: Elements of Reusable Object-Oriented Software](https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612)\r\n    - [ ] [Code: The Hidden Language of Computer Hardware and Software](https://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319)\r\n    - [ ] [Refactoring: Improving the Design of Existing Code](https://www.amazon.com/Refactoring-Improving-Design-Existing-Code/dp/0201485672)\r\n    - [ ] [The Pragmatic Programmer](https://www.amazon.com/Pragmatic-Programmer-Journeyman-Master/dp/020161622X)\r\n    - [ ] [Code Complete: A Practical Handbook of Software Construction](https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=pd_lpo_sbs_14_t_1?_encoding=UTF8&psc=1&refRID=K75WSC0JK6J62XWX4AHR)\r\n    - [x] [Real Programmers Don't Use PASCAL](http://web.mit.edu/humor/Computers/real.programmers)\r\n- [x] Work with a dataset larger than one petabyte\r\n- [x] Write your own ML model using nothing but a math library (numpy or equivalent)\r\n- [x] Throw away code for a project and start from scratch\r\n- [ ] Work in an open-space environment\r\n- [x] Work in an office/cubicle environment\r\n- [x] Work for a tech giant\r\n- [ ] Work for a startup with < 10 engineers\r\n- [ ] Work as a manager for a dev team\r\n- [ ] Write your favorite game in any language\r\n- [x] Write a mobile app specifically for one platform\r\n- [ ] Write a cross-platform mobile app\r\n- [ ] Work as a freelancer\r\n- [ ] Teach a kid to code\r\n- [x] Teach a class on programming\r\n- [ ] Write and publish a technical book\r\n- [ ] Receive a job offer without an interview\r\n- [ ] Publish a technical tutorial\r\n- [ ] Answer a question on StackOverflow\r\n- [ ] Have an answer upvoted 100+ times on StackOverflow\r\n- [ ] Write a program using strictly Vim or Emacs in the terminal\r\n- [ ] Write a non-trivial program in:\r\n    - [x] C\r\n    - [x] MIPS\r\n    - [ ] Go\r\n    - [x] C++\r\n    - [x] C#\r\n    - [x] Node.js\r\n    - [x] TypeScript\r\n    - [x] Python\r\n    - [x] Java\r\n    - [x] Android\r\n    - [ ] Swift\r\n    - [ ] Rust\r\n    - [ ] Elixir\r\n    - [ ] Scala\r\n- [ ] Write a program in a functional language\r\n- [x] Write a program for a robot\r\n- [ ] Work on software that requires government clearance\r\n- [ ] Infiltrate a large system undetected\r\n- [ ] Place top 5 in a Kaggle competition\r\n- [x] Write a blog about your career ","data":{"title":"My Software Dev Bucket List","date":"2018-8-31","path":"bucketlist"},"isEmpty":false,"excerpt":""},"joiningmicrosoft":{"content":"\r\nI've had a few friends ask me what my journey to Microsoft was like and why I chose to work here. Microsoft, like the Yankees and black licorice, is one of those \"love it or hate it\" kind of things (I fall emphatically into the latter camp for both of those other two). Everyone has reasons or experiences that make them feel one way or the other. As for myself, I've really enjoyed working at Microsoft, so I thought I'd document a little bit of my journey and why I ended up where I did.\r\n\r\n## The Interview\r\n\r\nI first interviewed with Microsoft in 2015 for a summer internship. When they called me and asked me to fly out for a round of final interviews in Redmond, my wife and I saw it as a free trip to Seattle. In my head, I was nowhere near qualified to work at a place like that and had all but written myself off from the start. The [Imposter Syndrome](https://en.wikipedia.org/wiki/Impostor_syndrome) was as real as ever. But I had promised my wife earlier that year that I would take us some place out of Utah that summer for an internship of some kind, so I really did want it to work out.\r\n<!-- \r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/Microsoft/sign.jpg?raw=true) -->\r\n\r\nI didn't feel great about my interviews - a couple of them felt downright terrible. But I did try hard to explain my thought process and walk my interviewers through my solutions to the problems they presented. \r\n\r\nAfter our brains had become sufficiently scrambled through the 4-hour interview process, the recruiters took us to lunch at the Commons. When we came back from lunch, they handed out Microsoft hoodies and swag (pretty much a participation trophy for interviewing, but **score** nonetheless). I was just about to head out the door to go back to the hotel, intent on enjoying the rest of the weekend in Seattle with my sweet wife, when one of the recruiters pulled me aside.\r\n\r\nWe went into a nearby conference room with one of the other recruiters, where they explained that they had an offer on the table for me. Shock doesn't begin to describe what I was feeling. It was cool to see that they were genuinely excited for me. I think they were even more excited for me to tell my wife. \r\n\r\nOn principle, I wouldn't officially accept anything until I had talked to Kate first, so I told them I would send them an email later that day. I called Kate as soon as I left the building and could barely contain my excitement as I tried to explain what just happened, not really knowing the answer myself. We spent the night in downtown Seattle, fully able to relax and imagine what our summer would be like in this new place.\r\n\r\n<!-- ![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/Microsoft/seattle.jpg?raw=true) -->\r\n\r\n## A New Intern\r\n\r\nThe day after I took my last final in May of 2016, we packed up our Highlander with everything we'd need for the next 3 months and made the 12-hour drive to our summer home. Microsoft put us up in an apartment in Redmond, which was close enough to one of the buildings that I could schedule a shuttle to pick me up and take me to the office. I became friends with many of the shuttle drivers I met that summer and remain very close friends with a few of them to this day. I've been to dinner with them, attended funerals of family members, and even stayed in one of their houses while I was looking for a place to stay for my family. Some of the finest people I've ever had the chance to meet.\r\n\r\nI spent that first summer in building 109 of Microsoft's campus, working with the WDG Global Services Localization Data Insights team (kind of a mouthful). My project was to automate the process by which our team identified meaningful feedback, particularly when it came to localization issues in Windows. I was able to work with a few teams across Microsoft to create a system that utilized an internal big data platform and natural language processing/machine learning tools to process and identify feedback most likely to be actionable in order to pass it along to developers. \r\n\r\n<!-- ![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/Microsoft/project.jpg?raw=true) -->\r\n\r\nIt solved a critical business need for the team, as they had previously been hiring sub-contractors to _read each piece of feedback, one by one_ and decide which ones were actionable. That may work for a smaller system, but for something like Windows, that's just not scalable. The project was a success, I learned a ton and had a blast doing it. For more detail on this project, check out my [blog post](/Microsoft-Summer2016/) written right after finishing for the summer. Of course, I enjoyed the many intern events and parties held for us... A private Ellie Goulding concert at the Space Needle where they hand out new Surface Books at the end wasn't anything to sneeze at... But the highlight for me was the learning and growing I was able to do, being surrounded by an amazing group of talented individuals that didn't mind taking the time to teach and mentor an intern.\r\n\r\n<!-- ![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/Microsoft/team.jpg?raw=true) -->\r\n\r\nI accepted an offer to return to the same team the following summer. As these things go, there was a massive re-organization within the team, my manager left Microsoft to pursue a personal dream of civic engagement as the City of Seattle's new Open Data Program Manager, and I would be reporting directly to my previous skip-manager. Although I was very sad to hear my manager would be leaving, I was happy to still be working under someone I knew and respected a great deal. \r\n\r\n## Return of the Intern\r\n\r\nOur journey back to Redmond after a long school year was pretty much the same, aside from one major detail - my wife was 6 months pregnant. She was due on August 10th, just 13 days after I was scheduled to finish my internship. Microsoft was _extremely_ accomodating with us. They put us in an apartment that was literally a 6-minute bike ride from building 109. They told me that if anything came up, I should feel free to take time to be with her and not to worry about work. My recruiters even got us a bag of gifts for the baby from the Microsoft store. We really felt that they had our interests at heart and cared about our little family.\r\n\r\n<!-- ![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/Microsoft/baby.jpg?raw=true) -->\r\n\r\nMy project during that second summer was very broad, more of a question to answer with no real guidance given on the implementation. I was free to take the project in whatever direction I saw fit. The question was \"How do we identify influential communities of Windows users/devices so that we can weight their feedback more heavily?\" These communities went much deeper than just \"gamers, business users, mobile users, etc.\" We already knew that for most devices. I wanted to find a way to use Windows telemetry to cluster devices together that actually *behaved* in a similar way, so that we could amplify the voice of Windows Insiders (users that receive Windows versions before they are released to the public) that more accurately represented that population.\r\n\r\nAt a 30,000 foot level, my approach was fairly straight forward: run k-means clustering on Windows devices within a locale (primary language used on device) and assign each device a score proportional to the size of the cluster and inversely proportional the device's distance from the centroid (higher score = big cluster and close to center). The problem became finding ways to run this efficiently on pedabytes of telemetry data and setting it up to run in an automated workflow. The systems we used had a timeout of 2 hours when running a script, so in order to give it time to finish, I was forced to break it up into smaller chunks of data and run many smaller scripts in parallel.\r\n\r\nThe project went well, and laid a foundation for future work in the area. But as with any probabilistic problem (especially when dealing with unsupervised learning), it's difficult to know when you've found the right approach. Testing your solution can be just as hard as developing it in the first place. When it came down to it, 12 weeks just wasn't a lot of time to understand the problem, map out a solution, implement it by myself and verify the results. And when I say \"by myself,\" that's not to say I didn't receive a lot of help throughout the process. I just mean that no one else was _directly_ working on the project with me. I'm confident that given a few more months, we would have come up with a solid approach to answer this question.\r\n\r\n## The Decision\r\n\r\nAt the end of the summer, I had some difficult decisions to make - whether or not to stay at Microsoft and, if I did stay, whether or not to stay with my current team. I really had enjoyed working with these people for the past two summers. I had learned a lot and was sure there was more to learn from them. But I also felt like broadening my horizons and looking at what other teams/companies had to offer. Two of the main things I was looking for were opportunities to challenge myself and grow, as well as a family-friendly atmosphere.\r\n\r\nThroughout the summer, I had been trying to follow the advice of mentors to keep my options open, so I interviewed with several different companies and had informationals with several teams across Microsoft. In that process, I discovered the Partner Catalyst Team, whose charter was basically to code with Microsoft partners on whatever they were building, which often times included contributing to open-source projects and traveling around the world. I met with a couple of the managers from the team, and even jumped on a Hackathon project with one of them as a little test run of what it would be like to work with them. I had a *great* time and learned a lot in the 3 days we worked together.\r\n\r\nWe went home for the summer. I was still talking to and interviewing with several different companies around the West, including leaving my poor wife while she was 8.9 months pregnant to fly to Silicon Valley. After weighing a few options, thinking about what was best for our family and where I could grow the most, I chose to stay at Microsoft, but switch to this new team. Both my wife and I thought this was the right call and we were ecstatic.\r\n\r\nI went to school and finished my last semester, graduating in December of 2017 and returning to Microsoft on January 8th of 2018. The team had been re-organized and renamed as Commercial Software Engineering (CSE), and I'd be specifically working with the Digital Win Room team within that organization. I spent a week with the team, and then took 2 of the 3 available months of paternal leave, which I was able to use at any time before our new son Jack's birthday in August. I couldn't believe how understanding they were and that the team was happy to let me take that time to get the family moved and spend time together before really starting at work.\r\n\r\nI've been officially back for just about 3 months now, and I have had a blast with some really cool projects and fun adventures. I'll do my best to keep the blog updated on the happenings here.\r\n\r\nSo, there you have it. I joined Microsoft because I felt like it would be a place where I could grow as a Software Engineer and, even more importantly, as a husband and father. I stay because those things are still true. \r\n","data":{"title":"Joining Microsoft","date":"2018-6-1","path":"joiningmicrosoft"},"isEmpty":false,"excerpt":""},"microsoft2016":{"content":"\r\nIn 2016, I had the opportunity of working with the WDG GS Data Insights team at Microsoft. While there, I was given the task of automating the process of taking action on international customer feedback. Because Windows 10 gets such a high volume of feedback from its users, a lot of valuable information sits idly in a database without anyone being able to act on it. Of course, there is always a lot of junk to sift through and LOTS of duplicated pieces of feedback, which made the challenge of automating the process even more interesting.\r\n\r\nI was privileged to work with teams and individuals across Microsoft, many of whom were quite a few pay grades ahead of us lowly interns. One of my absolute favorite parts of my internship was the aspect of collaboration. As my all-time hero once said, \"It's amazing how much can be accomplished when no one cares who gets the credit.\" (John Wooden) While I understand and agree that everyone deserves credit for their work and accolades for a job well-done, I also feel that there is something to be said for those synergistic teams that are able to feed off of one another and do something great because they are more focused on the thrill of the project than their own career advancement.\r\n\r\nI was grateful for my Database Systems course I had taken the previous semester, because I was able to help optimize the database being used. Previously, some of the queries took over a minute (sometimes more) to process. After some of our optimizations, those queries were running in 2 seconds or less.\r\n\r\nIn order to identify actionable feedback and filter out those that had nothing , I worked with a team that has spent the last 2.5 years developing an internal text analytics engine that could help do just that. I met with their team lead on an almost-weekly basis to discuss needs of the project and even help them debug their solution. It was a great partnership and I was grateful for all of their help.\r\n\r\nI also worked with several web technologies, as bugs and Work Items are housed in Visual Studio Online. In order to pass actionable feedback onto developers, I needed a Restful Web Api that I could call to \"promote\" such a bug. Thankfully, there was another team of very helpful individuals that managed that Api, and that ended up being the easiest part of the project.\r\n\r\nEven after launching our first batch of auto-promoted bugs, the system was not perfect. There were still a few pieces of feedback that probably should not have been promoted, but to give you some perspective, our first batch was only 131 promoted bugs out of 10M+ pieces of feedback sitting in the database. The filtering was not perfect, but it was a start.\r\n\r\nThe rest of the summer was spent trying to refine the process. I worked with teams of native-language speakers to identify the original translation of each feedback to determine the meaning. One of the great eye-openers during this process was the discovery of how difficult machine translation really is. It's an incredibly fascinating problem, but one that seems to have no perfect solution... yet.\r\n\r\nI've accepted an offer to return to the same team for another internship next summer, where I'm told I might be working on helping to improve machine translation. I'm still not sure in what capacity that will be, but I'm just excited to be a part of it.\r\n","data":{"title":"Microsoft - Summer 2016","date":"2016-9-2","path":"microsoft2016"},"isEmpty":false,"excerpt":""},"mistakes":{"content":"\r\nNow that I've click-baited you into opening this post, let me just reassure you of a few things:\r\n\r\n1. I still have my job\r\n2. The Azure subscription was just a development subscription, so no $ was lost\r\n3. An irresponsible amount of GIFs will be used in this post.\r\n\r\n![alt text](https://media.giphy.com/media/4PT6v3PQKG6Yg/giphy.gif)\r\n\r\n## The Discovery\r\n\r\nIt was a crisp, autumn work-from-home kind of Friday. After having breakfast with my family, I went downstairs to my office and cracked open the laptop to begin another wonderful day of writing code...\r\n\r\nThe afternoon prior, I had deployed an Azure Function that would be running a [Cloud Custodian](https://cloudcustodian.io/) policy to clean up our Azure Subscription. We had a lot of test resources that needed to be removed, so I asked our team to tag any resource groups they needed with `CreatedBy` and their email address for notifications. They all looked through the subscription and tagged their resource groups accordingly. I told them that early Friday morning, I would run a scrub of all resource groups and delete any that did not have owners, and that was the policy that was deployed... or so I thought.\r\n\r\nThat morning, to my utter shock and horror, I opened the Azure portal to discover _**one**_ resource group... `cloud-custodian`.\r\n\r\nSo many questions started racing through my mind. How did this happen? Where did I go wrong? What had caused this lone surviving resource group to cannibalize all of his innocent, appropriately-tagged brothers? Panic had already begun to take over.\r\n\r\nI opened up the repository containing the policies I had written, and that's when I saw it. I had neglected to update the test tag I had been using... `CreatorEmail`. I had also been testing a policy that would send a weekly email to the members of my team with a summary of their resource groups, which would enable them to do a quick scan and go remove any they didn't need anymore. I created the `CreatorEmail` tag to test the notification system on the `cloud-custodian` resource group so that I wasn't spamming my whole team while I was testing the policy. At the end of the day, I ran a script that deployed all the policies I was working on. That test tag had been copied and pasted to the other policies had deployed. I did read things through, but clearly not closely enough. I had deployed a rogue agent... I opened the doors wide open and let a monster into our house.\r\n\r\n## The Fallout\r\n\r\nClearly, this was not going to be a mistake that would fly under the radar, so I figured the sooner I got in front of this, the less likely my head would end up on a pike outside the Microsoft headquarters. I immediately called my manager to tell him what happened. \r\n\r\n![alt text](https://media.giphy.com/media/sS8YbjrTzu4KI/giphy.gif)\r\n\r\nHe was in another call, so we chatted online. I told him what had happened. I'm not sure he believed me until he opened the portal and witnessed the devastation for himself. _\"You're right... it nuked everything,\"_ he responded.\r\n\r\nAfter what seemed an appropriate moment of silence for the fallen resource groups, he was quick to point out that `a)` this is only a dev subscription and `b)` anything in the sub should be easily reproducible. He reminded me that everyone goes through something like this at some point, and this was a pretty low-stakes environment for it to happen. Then, like a mayor of a city ravaged by a natural disaster, he stated, _\"We can rebuild.\"_\r\n\r\nAfter the call with my manager, I called a couple team members to assess the extent of the damages. I called my tech lead and told him what had happened. He laughed. He told me about a time where he had deleted a production database early on in his career and reminded me of some of the same things my manager told me. We then made a plan for restoring some of the lost resource groups that were actually public facing.\r\n\r\nI decided it was time to tell the team. I wrote up a long email describing the details of what had happened and posted it in our chat channel as well. I was fully expecting a swift and brutal judgement worthy of my crimes.\r\n\r\n![alt text](https://media.giphy.com/media/qiDb8McXyj6Eg/giphy.gif)\r\n\r\nBut rather than _\"How could you?\"_, _\"How dare you?\"_ or other responses you might expect in such a circumstance, every member on the team simply reacted to my message with a single emoji:\r\n\r\nðŸ’ª\r\n\r\nas if to say _\"Be Strong.\"_\r\n\r\nAnd that was pretty much it. After some of the initial rawness of the incident had worn off, we were able to laugh about it a little bit. A team member pointed out that until you break production, you're not a real engineer. I countered with the remark that if that was all it took, I would've done this a long time ago. I got to work restoring the damages, reached out to other developers that were using some of our deployed services and helped them get back up to speed.\r\n\r\n## Takeaways\r\n\r\n### Technical Empathy and a Blameless Culture\r\n\r\nOne of the surprising parts of this experience was the empathy shown by my colleagues. They were quick to point out times where similar things had happened to them or others and to remind me that it was an honest mistake. This kind of \"technical empathy\" is not only helpful, but essential in building successful, collaborative software teams that trust each other and work together. This is one example where the empathy of my teammates gave me the boost that I needed to get back up and start fixing things rather than wallow in my own self pity.\r\n\r\nTechnical empathy should be used in more than just response to disasters. For example, it is also useful in giving (and receiving) code reviews. Rather than belittling team members for making what we perceive to be a mistake in the code, we can try to understand why they felt they had to do what they did. There very well may be an obstacle that we, as reviewers, are unaware of. Technical empathy should also be used in reading legacy code. In our team at Microsoft, we work with dev teams of other companies to help them solve interesting challenges, usually related to Azure. As a byproduct of that, we get to see _a lot of new codebases_. Every time we crack one open, there's a little bit of apprehension (and even fear) about what we might find. Technical empathy can be applied here too. A popular article titled [\"The software engineerâ€™s guide to asserting dominance in the workplace\"](https://medium.com/feature-creep/the-software-engineer-s-guide-to-asserting-office-dominance-ddea7b598df7) (satirically) recommends the following strategy for ramping up on a codebase when joining a new team:\r\n\r\n_\"Spend the rest of the day familiarizing yourself with the teamâ€™s codebase. Every five to ten minutes, let out a deep sigh and write something down on a notepad. Maintain a demeanor of mild disgust on your face that gets increasingly more annoyed as you browse through more and more of the code. Mumble words like â€œrefactorâ€ and â€œrewriteâ€ under your breath. Start drawing random complex architectural diagrams on your whiteboard. By 3 PM you should be visibly angry. Eat some chili peppers to force yourself to sweat. At 4 PM, allow your rage to boil over and throw your last egg at the wall in a fit of rage. Slam your laptop closed and head home early._\r\n\r\nIt is easy to be critical when you're unaware of the constraints and difficulties encountered by other developers. Often times, on high-performing teams, if you have an idea within the first few seconds of looking at a problem, it's possible that other engineers might have tried the same approach. That's not to say you shouldn't share your ideas on how things could be better, but be hesitant about jumping to the conclusion that you're surrounded by morons and that you are the only one who is truly \"one with the code.\" For more info on the subject, visit [this article on how empathy is a technical skill](https://www.infoq.com/articles/empathy-technical-skill/).\r\n\r\nRather than becoming upset and demanding my immediate dismissal, my team responded with _\"Sorry that happened, it happens to all of **us**, what do we do to fix it?\"_ The phrase \"blameless culture\" became more than just a line in our team's working agreement.\r\n\r\n### Use the Buddy System\r\n\r\nFor any of you that ever went to any kind of summer camp, you know that one of the first rules they tell you is to never wander off alone. Bring a buddy along with you. If I would have just listened to my camp counselor and followed that simple rule, you wouldn't be reading this blog post right now.\r\n\r\nAs software engineers, asking others for help or to look over our work isn't always our natural inclination. We often see ourselves as a one-man army, equipped with a mechanical keyboard and multiple monitors, leaving a trail of dead bugs and shattered features in our wake.\r\n\r\n![alt text](https://media.giphy.com/media/TBOvwBGkQShnq/giphy.gif)\r\n\r\nTo counteract that behavioral tendency, we put up quality gates. On our team, in every repository we work with, we establish branch protection policies that prohibit anyone from pushing directly to `master` or even `dev`. We require pull requests that trigger CI pipelines and establish baseline coverage requirements for both the project as a whole and the current diff being submitted. (Side note on code coverage: We recognize that developers can \"game the system\" when it comes to code coverage, so the tests are reviewed with just as much scrutiny as the application code.) \r\n\r\nThese standards are required on every single project that we work on. All of this could have been so easily prevented if I had just asked for even one other pair of eyes to look over these basic policies before I had deployed them. Instead, I ignored the rules, wandered off into the wilderness by myself and got mauled by the proverbial bear of disaster.\r\n\r\nUPDATE: Now, these policies are deployed from a central build server via an Azure Pipeline, which is triggered on merges to the master branch of a repository that requires two approvals from members of the team in order to merge.\r\n\r\n### Be especially careful with irreversible actions\r\n\r\nPart of the tragedy of this episode was its finality. There was no commit to revert, no deployment to roll back. These resource groups were as lost as Black Widow after Endgame.\r\n\r\nA lot of what we do as software engineers is set up guard rails and fail-safes so that when things break, we have a quick way to return to the state we were in before the change. When we decide to venture outside the lines and do things that are irreversible, we should do so with care and hopefully, with a buddy â˜.\r\n\r\nUPDATE: Currently, in our Cloud Custodian policies, \"nuclear\" actions (like delete) are given a \"grace period\" of 7 days before they are enacted. The resources are tagged for the operation and an email is sent to the team member(s) responsible for the resource so that they can cancel the operation if necessary.\r\n\r\n### Automate, Automate, Automate\r\n\r\nFor several members of my team, this loss was almost trivial. Many of the resource groups were just a CLI execution or a pipeline run away from being restored. Sometimes we spend _too_ much time trying to automate tasks when we just need to stand something up and get unblocked, but when possible, use scripts, pipelines and templates to do the dirty work. It will save you (and probably your team) hours of work later on if that work needs to be replicated. It will allow you to focus on solving the cool problems instead of re-figuring out something you've done before.\r\n\r\n## Conclusion\r\n\r\nMistakes are part of software engineering and a part of life. I make them every day, and I don't expect that to stop any time soon. But I don't expect to make the same mistake twice. My all-time basketball hero, John Wooden, once said _\"Failure is not fatal, but failure to change might be.\"_ You don't have to let something like this happen to you. In fact, I highly recommend that you don't. Learn from my experience. Ask for another pair of eyes. Be careful if you're doing something irreversible. Set up recovery steps to back up your work. It will save you a lot of heartache and a lot of time someday.\r\n","data":{"title":"I Wiped Our Entire Azure Subscription...","date":"2019-10-4","path":"mistakes"},"isEmpty":false,"excerpt":""},"nbaclustering":{"content":"Player positions in the NBA have become a rather fluid concept. Teams like the Warriors with their \"Lineup of Death\" have shaken the traditional mindset of the basketball world. We wanted to be able to build out a clustering model that used a player's statistics to identify the player's \"true position.\" When we say \"true position,\" we mean the position the player plays most alike. While LeBron James could be listed at just about any position on the floor, we wanted to know what his stats told us. By creating an unsupervised clustering model, players would be grouped together with other players of a similar statistical model.\r\n\r\n## Introduction\r\n\r\nOur Data Mining project was based on looking for statistical groupings in the National Basketball Association that define the different positions in the modern game of basketball. In basketball, often a given position becomes an argument for what will and wonâ€™t work on a roster, when itâ€™s really much more complicated than that. We want to define the numbers behind what a guard, forward, wing or center is, as well as look for outliers, such as forwards performing statistically equivalent to guards. \r\nThe key idea to our project was that there is more to a player than his labeled position. As the NBA has changed in the modern game, there has been a tendency to â€˜play smallâ€™ as popularized by the Golden State Warriors infamous â€˜Lineup of Deathâ€™. Examples like these have shown that players longtime slotted into a single position are actually more flexible, and possibly more effective, when placed somewhere completely new. Our goal was to use clustering methodology to look for what defines each of the four positions and search for specific groups of players, as well as outliers, in order to view the game differently. Our graphs have been created using Power BI, an interactive data visualization tool, and we strongly recommend opening the dashboard side by side while reading this paper. It can be found at [this link](https://app.powerbi.com/view?r=eyJrIjoiODgzZjhiMmQtYmU5My00NzM4LTk1MjUtNWVhYWUzM2RiYzhlIiwidCI6Ijg0YzMxY2EwLWFjM2ItNGVhZS1hZDExLTUxOWQ4MDIzM2U2ZiIsImMiOjZ9). \r\n\r\n## Data Wrangling and Cleaning\r\n\r\nFor our project, we used individual statistics of players from the 2010 - 2016 seasons in the NBA, which we obtained from https://probasketballapi.com/. The data didnâ€™t turn out to be perfect and required several rounds of cleaning and refining in order to produce something worth using. To start, there were several non-basketball athletes in the data set, many players were minimum impact competitors who hardly played and didnâ€™t last very long in the NBA, and some categories, like position, were labeled in an extremely haphazard manner. For example, a player could easily be labeled as a Guard, Point Guard, Shooting Guard, Point Guard/Shooting Guard or Shooting Guard/Point Guard with no distinction as to why any particular choice was made. In order to give a wide range of options, we ran our simulations over statistics gathered from each season, as well as an averaged statistical output from the entire 2010 - 2016 data set.\r\n\r\nIn cleaning our data, we eliminated all non-basketball players, required that a player has played at least 20 games and averaged at least 10 minutes played per game. To normalize the position labels and make up for a lack of distinction between some positions, we organized the athletes into four basic positions. â€œGuardâ€ consists of players labeled Guard, Point Guard or capable of playing Point Guard or Shooting Guard. â€œWingâ€ consists of Shooting Guards (only 3 players in the whole dataset labeled as such), hybrid Guard/Forwards and Small Forwards. â€œForwardâ€ consists of hybrid Small and Power Forwards and pure Power Forwards. And â€œCenterâ€ is our big man group, consisting of hybrid Power Forward/Centers and Centers. While originally, we only had three positions defined, looking at the numbers in the many categories provided by our data source, we felt these four categories most naturally used the distribution to create a good foundation for our analysis and represented the current state of NBA tactics. One of the difficulties in identifying effective clustering measures was the skew in number of players for each position. This is shown in the already-large and increasing number of guards, as depicted below.\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-1.png?raw=true)\r\n\r\nThis increase of smaller players seems to agree with the rest of our analysis, as the league is trending towards players who can score from long range as opposed to the traditional, â€œinside-outâ€ philosophy, but it adds to the challenge of finding effective clustering measures.\r\n\r\n## Initial Analysis\r\n\r\nWe began our analysis by doing simple comparisons among the four positions we had identified. We compared the averages of the box score statistics, the advanced statistics and some of the shot chart statistics to look for simple ways in which the positions differentiated themselves.\r\n\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-2.png?raw=true)\r\n\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-3.png?raw=true)\r\n\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-4.png?raw=true)\r\n\r\n## Clustering\r\n\r\nWe began with clustering our dataset using both hierarchical clustering with single-link distance metrics and assignment-based clustering using k-means++ and Lloydâ€™s algorithm. We originally used Gonzalezâ€™s as well but found it less effective and switched our comparisons to single-link and k-means++. For both algorithms, we clustered the data from k = 3 to k = 8, using every combination of 7 statistical feature sets for each player: box score, advanced statistics, shot zone, shot range, shot area, action type and shot type. We ran assignment-based clustering for larger values of k, but with the amount of time required to run single-link hierarchical clustering, we limited our comparison from k = 3 to k = 8. Even with the limited scope, this resulted in almost 10,000 different clusterings with no simple way to identify which were better suited for our purposes. In order to find which clusterings best represented our chosen positions, we began searching for â€œpolarityâ€ among the results. We define â€œpolarityâ€ for a group of clusters as the average percentage of the dominant position for each cluster. Ideally, we would want a group of clusters to have a polarity of 100%, which would mean each cluster would consist entirely of one position.\r\n\r\n## Results\r\n\r\nThe first thing we found is that hierarchical clustering with single-link did not perform as well as we thought. We had anticipated that single-link would do well in linking the most statistically similar players one at a time and that this would lead to a more linear clustering of players. What it actually did, in most cases, was produce (K - 1) singleton clusters and 1 large cluster of the rest of the players. This lead to it getting great scores on our polarity tests, because a cluster consists of only 1 or 2 players, itâ€™s pretty easy to get a cluster of 100% the same position. The variation in cluster size is shown in the graph â€œCluster Size Standard Deviation.â€ While not especially valuable for clustering, it was interesting to see the algorithm identify the gameâ€™s â€œsuperstarsâ€ (Russell Westbrook, Lebron James, Kevin Durant, etc.). We then decided to programmatically prioritize cluster sets that had larger chunks of one position in each collection and focus solely on k-means++ assignment-based clustering for our results.\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-5.png?raw=true)\r\n\r\nIn the end, our polarity methods determined that the best clustering result was using Lloydâ€™s algorithm with k-means++, clustering on box score, advanced statistics, shot range (Less than 8 ft., 8-16 ft., 16-24 ft., 24 ft.+), action type (pull-up jumper, alley-oop dunk, etc.) and shot type (2 pt. vs. 3 pt.) for the 2013 season data set where k = 7.\r\n\r\n![alt text](https://github.com/tbarlow12/tbarlow12.github.io/blob/dev/resources/images/nba-6.png?raw=true)\r\n\r\nIn figure 5, you can see the basic results of our determined best clustering, organized by position and each clusterâ€™s size. We call cluster 1 the â€œAttack the Rimâ€ cluster. It consists of high volume inside shooters like Derrick Rose, Kobe Bryant, Brook Lopez and JaVale McGee. Itâ€™s interesting to see how this clustering put players in very different positions into the same grouping. Cluster 2 is our â€œTrue Point Guardsâ€ (traditional, pass-first) collection, with Rajon Rondo, Jrue Holiday, Steve Nash and Eric Bledsoe leading the way. Cluster 4 is referred to as our â€œSpot-up Shootersâ€ cluster, consisting of high volume outside shooters like Jimmer Fredette, Brandon Rush, CJ McCollum and Jason Terry. Players who are known more for their ability to shoot from the floor, and are most likely subpar defenders. Cluster 7, which we call the â€œ6th Man Clusterâ€, is another intriguing look. It is full of guards known for high scoring and utility in limited minutes. Matthew Dellavedova, J.J. Barea, Jeremy Lin, Patrick Beverley, Lou Williams, Jerryd Bayless, Shelvin Mack, Iman Shumpert and even Andre Miller are all sorted here.\r\n\r\nWhile these little itemizations are fun, overall, we learned a great deal from this project. The first thing we learned was that more data is not inherently good thing. The more parameters you input, the more confounding and confused your results can become. As your results become harder to visualize, itâ€™s difficult to tell if your results actually mean anything. Clustering is also a difficult problem, and the methodology you decide on at the beginning heavily affects your results. In our quest to determine positional outliers, we also had tremendous success. One example is our â€œ6th Man Clusterâ€. While being primarily guards, an outlier is the inclusion of Andre Iguodala. Though this is based on the 2013 data set, it feels reminiscent of Iguodalaâ€™s run as a key playmaker in Golden Stateâ€™s aforementioned â€˜Lineup of Deathâ€™. His ability to play a role far from his position title led to his naming as Finals MVP in Golden Stateâ€™s championship. Looking to the future, similar clusterings on the 2016 data set have us extremely interested in the futures of Sam Dekker, Kelly Oubre Jr. and Myles Turner. It becomes even more difficult to cluster over a career when taking into consideration the evolution of each playerâ€™s style of play. One of the most well-known examples of this was Michael Jordanâ€™s shift from the high-flying dunk virtuoso that he was when he entered the league to the clutch scorer from mid-to-long range that he became near the end of his career. We donâ€™t think the NBA has ever been as simple as slotting 5 players into position on the floor. And itâ€™s only going to get more interesting.\r\n\r\n## Future Work\r\n\r\nIn order to narrow the scope of this project, we chose to cluster based on groups of statistics (box score, advanced, shot type, etc.) rather than individual statistics themselves. This is partly for our own sanity in trying to keep track of 123 different statistical measures for each player over the span of 6 NBA seasons, but mostly because of the combinatorial nightmare that would arise when trying to find effective combinations of statistics for high accuracy in clustering players based on position. Given the time and resources, we would like to run all possible combinations of those stats, or at least a reasonable amount of those combinations, to see if we can fine-tune our view about which statistics really are indicative of position, how those positions change and evolve over time and how the game is influenced by this position fluidity.","data":{"title":"NBA Position Clustering","date":"2017-4-8","path":"nbaclustering"},"isEmpty":false,"excerpt":""},"pycon2019":{"content":"\r\nThis is a summary of 5 of my favorite talks from PyCon 2019. I learned a ton throughout the conference and felt that the learnings needed to be shared. I've tried to summarize as best as I could from the notes that I took. I believe the talks will be available online soon if they are not already. Big thanks to the speakers for all the effort they put in to make their talks so practical and engaging.\r\n\r\n## Black Swans - Keynote from Russell Keith-Magee\r\n  \r\n  This was the first keynote of the conference, and it was **awesome**. Mr. Keith-Magee discussed the [black swan theory](https://en.wikipedia.org/wiki/Black_swan_theory), which, as a very crude summary, are things that seem obvious in hindsight but that no one had thought of previously. \r\n  \r\n  He tied that to the 1983 America's Cup winning sailing team from Perth, Australia, and their use of the [winged keel](https://en.wikipedia.org/wiki/Winged_keel). This innovation helped their boat move with less resistance. Sailboats had been roughly the same for the previous few decades and teams that failed to innovate were left behind. He invited us to challenge our assumptions and to look for \"black swan\" innovations, particularly relating to the open-source and Python communities.\r\n\r\n## Break the cycle: three excellent python tools to automate repetitive tasks - Thea Flowers\r\n\r\n### 1. `tox`\r\n    \r\nOne of the most common tools used in Python applications. Used to run tests in multiple environments and even multiple versions of frameworks. For example if you want your app to support multiple versions of Python and multiple versions of Flask, your `.ini` file could look something like (example taken from the `flask-restful` repo):\r\n    \r\n~~~ \r\n[tox]\r\nenvlist=\r\n    py{27,34,35,36,37}-flask{0_10,0_12,10}\r\n\r\ndeps =\r\n    flask0_10: flask>=0.10,<0.11\r\n    flask0_12: flask>=0.12,<1.0\r\n    flask10:   flask>=1.0,<1.1\r\n\r\n[testenv]\r\nusedevelop = true\r\ncommands =\r\n    pip install -e .\r\n    nosetests\r\ndeps =\r\n    -r{toxinidir}/tests/requirements.txt\r\n~~~ \r\n\r\nWhen the command `tox` is executed, this would run the test suite **15 times** (cross product of Python environments and Flask versions - `5 x 3 = 15`). As part of that process, it would install the necessary dependencies in virtual environments (according to each version) and run the tests. Pretty cool.\r\n\r\n### 2. `nox`\r\n    \r\nPretty cool to listen to a talk from the original `nox` author. `nox` is very similar to `tox`, but rather than using the `.ini` file, its configuration is done in Python itself. The `nox` equivalent to the `tox.ini` file above would be something like:\r\n\r\n~~~ python\r\n@nox.session(python=['2.7', '3.4', '3.5', '3.6', '3.7'])\r\n@nox.parameterize('flask', ['0.10', '0.12', '1.0'])\r\ndef tests(session, flask):\r\n    # Install pytest\r\n    session.install('pytest')\r\n    # Install version of flask\r\n    session.install(f'flask=={flask}')\r\n    # Install everything in requirements-dev.txt\r\n    session.install('-r', 'requirements-dev.txt')\r\n    # Install the current package in editable mode.\r\n    session.install('-e', '.')\r\n    # Run pytest. This uses the pytest executable in the virtualenv.\r\n    session.run('pytest')\r\n~~~ \r\n\r\nAlso a really cool option, which is helpful if you need something slightly more flexible than `nox` or if you'd rather write config-as-code.\r\n\r\n### 3. `invoke`\r\n\r\nInvoke is seen as a more flexible automation tool. For example (straight from `invoke`'s docs):\r\n\r\n~~~ python\r\nfrom invoke import task\r\n\r\n@task\r\ndef clean(c, docs=False, bytecode=False, extra=''):\r\n    patterns = ['build']\r\n    if docs:\r\n        patterns.append('docs/_build')\r\n    if bytecode:\r\n        patterns.append('**/*.pyc')\r\n    if extra:\r\n        patterns.append(extra)\r\n    for pattern in patterns:\r\n        c.run(\"rm -rf {}\".format(pattern))\r\n\r\n@task\r\ndef build(c, docs=False):\r\n    c.run(\"python setup.py build\")\r\n    if docs:\r\n        c.run(\"sphinx-build docs docs/_build\")\r\n~~~ \r\n\r\nwhich is run by calling:\r\n\r\n~~~ bash\r\n$ invoke clean build\r\n~~~ \r\n\r\nThis can be very useful even outside of things like Python testing. Planning on converting some of the scripts I use on my machine to `invoke` commands.\r\n\r\n## Wily Python: Writing simpler and more maintainable Python - Anthony Shaw\r\n  \r\nThis talk was definitely one of my favorites. We had just talked a lot about code complexity when working on [VoTT](), and not many of us knew exactly how that was calculated. Anthony talked about three different ways to measure code complexity and how all of them play a factor in calculating the \"maintainability index\".\r\n\r\nThe first, most crude way of measuring complexity is by **lines of code**. Fewer lines *can* be less complicated, but in a language like Python, you could have a one-liner like this function for the Sieve of Eratosthenes:\r\n\r\n~~~ python\r\ndef sieve_eratosthenes(n):\r\n    return sorted(set(range(2,n+1)).difference(set((p * f) for p in range(2,int(n**0.5) + 2) for f in range(2,(n/p)+1))))\r\n~~~ \r\n\r\nAnother measure is by **cyclomatic complexity**, for which he gave the example of ordering a Big Mac:\r\n\r\n~~~ \r\nCASHIER: What would you like?\r\nME: I'd like a Big Mac please\r\nCASHIER: Make it a meal?\r\nME: Sure\r\nCASHIER: Small, large or super size?\r\nME: Large\r\nCASHIER: What would you like to drink?\r\nME: Coke\r\nCASHIER: Diet or regular?\r\nME: Regular\r\n~~~ \r\nJust in that interaction, the cyclomatic complexity would be **5**, which is basically *the number of decisions that need to be made*. In Python, things like `if`, `elif` and `try` are things that increase cyclomatic complexity.\r\n\r\nThis is why you might get asked in a code review to \"invert `if` statement to reduce nesting.\" The phrase *\"flat is better than nested\"* is directly from the Zen of Python (discussed more below) and explained in the famous email to the Python mailing list [Why \"flat is better than nested\"?](https://mail.python.org/pipermail/python-list/2010-October/590762.html).\r\n\r\nWhere the code complexity measurement gets more \"mathy\" is when we start talking about Halstead Complexity measures. I won't go too deep into this, but it involves operators, operands, sums of each and much more.\r\n\r\nWhen you combine these three measurements, you can calculate the `maintainability index` as such:\r\n\r\n~~~ \r\nMI = 171 - 5.2 * ln(Halstead Volume) - 0.23 * (Cyclomatic Complexity) - 16.2 * ln(Lines of Code)\r\n~~~ \r\n\r\nThere have been variations of this original formula, which you can read about [here](http://www.projectcodemeter.com/cost_estimation/help/GL_maintainability.htm)\r\n\r\nAfter discussing the theory behind all of this, Mr. Shaw introduced the Python utility he wrote called `wily`, which you can install on pip. `wily` is \"A command-line application for tracking, reporting on complexity of Python tests and applications.\" Definitely planning on using `wily` in my next Python project!\r\n\r\n## The Zen of Python Teams - Adrienne Lowe\r\n  \r\nMany people are familiar with \"The Zen of Python\" as laid out in an email from Tim Peters and described on python.org as \"guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.\" (meaning the 20th is something for us as a community to fill in for ourselves)\r\n\r\nHere are the 19 aphorisms:\r\n\r\n~~~ \r\nBeautiful is better than ugly.\r\nExplicit is better than implicit.\r\nSimple is better than complex.\r\nComplex is better than complicated.\r\nFlat is better than nested.\r\nSparse is better than dense.\r\nReadability counts.\r\nSpecial cases aren't special enough to break the rules.\r\nAlthough practicality beats purity.\r\nErrors should never pass silently.\r\nUnless explicitly silenced.\r\nIn the face of ambiguity, refuse the temptation to guess.\r\nThere should be one-- and preferably only one --obvious way to do it.\r\nAlthough that way may not be obvious at first unless you're Dutch.\r\nNow is better than never.\r\nAlthough never is often better than *right* now.\r\nIf the implementation is hard to explain, it's a bad idea.\r\nIf the implementation is easy to explain, it may be a good idea.\r\nNamespaces are one honking great idea -- let's do more of those! \r\n~~~ \r\n\r\n(If you didn't know, these are in a Python Easter Egg. Fire up `python` in your terminal and type\r\n    \r\n~~~ \r\n>>> import this\r\n~~~ \r\nto see what I mean.)\r\n\r\nThese sayings are usually applied directly to the code that we write, but Adrienne Lowe discussed how we can take some of these principles and apply them directly to how we work within our teams. Here are a few that she discussed:\r\n\r\n### \"Beautiful is better than ugly\"\r\n\r\nWe can avoid \"acting ugly\" with our teammates. \"Acting ugly\" can come in the form of bitter, cutting code reviews, hoarding information and refusing to collaborate with others.\r\n\r\nShe referenced Westrum's [\"A typology of organisational cultures\"](https://qualitysafety.bmj.com/content/13/suppl_2/ii22), which discusses three different types of cultures in a team:\r\n\r\n#### 1. Pathological\r\n- Information is a **personal** resource (not to be shared)\r\n- Cooperation is discouraged\r\n- Failure leads to scapegoating\r\n- Accidents lead to blaming\r\n\r\n#### 2. Bureaucratic\r\n- Responsibilities are narrow\r\n- Alignment of team takes precedent over mission\r\n- Failure leads to seeking justice\r\n- Novelty leads to problems\r\n- Maintain turf\r\n- Insist on being done by the book - their book\r\n- Inter-team dynamics neglected\r\n\r\nThis one reminded me immediately of the old cartoon depicting Microsoft's organizational culture-of-old:\r\n\r\n![alt text](https://dougbelshaw.com/blog/wp-content/uploads/2013/09/organizational_charts.png)\r\n\r\nThankfully, I can say that in my ~2 years working for Microsoft, I have yet to experience that kind of culture. Things have changed :)\r\n\r\n#### 3. Generative (the goal)\r\n- High cooperation\r\n- Risks are shared\r\n- Failure leads to inquiry\r\n- Information flows freely\r\n- How can we accomplish our goal (\"we\" is expansive and inclusive of all)\r\n\r\n### \"Explicit is better than implicit\"\r\n- We should *always* have playbooks, documents, resources, onboarding guides and steps to take when confused\r\n- Having these in place and other process documentation makes it easier to include others and speeds up the work\r\n- It is better to keep conversations about code in **main channels** of Slack or whatever messaging service you use as opposed to DMs or other private places. Helps everyone benefit from the knowledge being shared\r\n- Documenting process also improves relationship between teams\r\n- We should **document our people**\r\n    - I enjoy working on...\r\n    - I get excited by...\r\n    - I struggle when...\r\n    - I feel appreciated when...\r\n    - I prefer feedback...\r\n    - Ask me for help with...\r\n- Helps with process of working within teams an can be extremely valuable\r\n\r\n### \"Simple is better than complex\"\r\n- We build meaningful relationships with small interactions that increase understanding and trust\r\n- Take time to have coffee with colleagues, catch up on weekend, etc.\r\n- **Remote teams** -> Remote Happy Hours - just jump on a video call to chat about lives\r\n- Build trust and familiarity with colleagues\r\n- Like software, we build relationships with small but meaningful actions\r\n\r\n### \"Errors should never pass silently\"\r\n- If something is wrong with my code, I want to know\r\n- If I do something to hurt someone, I want to know\r\n- We rely on other humans to know that we hurt them\r\n- Feedback is the tool we have to understand our impact on others\r\n- On healthy teams, people should understand what things they need to do to improve\r\n- Be careful about how you respond about mistakes. We all need to be open about mistakes and willing to \"share our trash\" to the point that we're not self-conscious about getting feedback from others.\r\n\r\n### \"In the face of ambiguity, refuse the temptation to guess\"\r\n- Don't `git blame` and stew about it\r\n- Assume the best and don't guess at motives.\r\n- Ask where they're coming from and try to understand why they did what they did\r\n- Be assertive\r\n- Open issues, comment on PRs\r\n- Challenge directly but care personally\r\n- Don't explain away code... or people\r\n- Especially for managers. Don't guess about how your direct reports are doing. You should know.\r\n\r\n### \"Now is better than never\"\r\n- Take **some** action to move closer to our goal\r\n- \"Doing and being wrong is a lot better than not doing at all\"\r\n- Everyone benefits from being reminded that they can start where they are\r\n- **Challenge** - Fill in the 20th line of the Zen and share it on social media with `#HereYaGoGuido`\r\n\r\n\r\n## Type hinting and `mypy` - Bernat Gabor\r\n\r\n- Why use types in Python? Why not just use Java/C#?\r\n  - Makes code easier to:\r\n    - understand\r\n    - use\r\n    - maintain\r\n    - debug\r\n    - refactor\r\n  - Creates more accurate code suggestions\r\n  - Does lint checks that find bugs with no tests\r\n  - Improve documentation\r\n  - Built-in data validation\r\n  - Performance increase (sometimes)\r\n    - `mypyc` - Compiles c-extension type hinted code (not full syntax support yet), can lead to 4 to 20x performance improvement (due to the avoidance of hashtable lookups)\r\n- You can *gradually* introduce typings into your code (not all or nothing)\r\n- Typing example:\r\n~~~ python\r\ndef greeting(name: str) -> str:\r\n    s: str = name # You can add type annotations inline\r\n    return s\r\n~~~ \r\n- Gotchas\r\n  - If you have to maintain both Python 2 & 3, this will be difficult\r\n  - If you have mulitple return types, the best way to use typings will be to use the `@overload` decorator and declare the function multiple times\r\n  - Type lookup - looks for type in closest namespace\r\n    - For example, if you have a function named `float` that returns a `float` data type, it will type the return value as the function itself\r\n- Typing packages\r\n  - `mypy` - Reference implementation type checker\r\n  - `pyre` - Facebook\r\n  - `pytype` - Google\r\n  - `pyright` - Microsoft (fun fact - written in TypeScript)\r\n\r\n## Conclusion\r\n\r\nI hope this summary was of value to someone. Thank you again to the speakers and organizers of PyCon 2019.\r\n","data":{"title":"Top 5 (unordered) Learnings from PyCon 2019","date":"2019-5-6","path":"pycon2019"},"isEmpty":false,"excerpt":""},"serverless-part1":{"content":"\r\n## Overview\r\n\r\n(_See the [original post here](https://serverless.com/blog/serverless-azure-functions-v2)._)\r\n\r\nWith the [recent updates to the `serverless-azure-functions` plugin](https://github.com/serverless/serverless-azure-functions/blob/master/CHANGELOG.md), it is now easier than ever to create, deploy and maintain a real-world REST API running on Azure Functions. This post will walk you through the first few steps of doing that. \r\n\r\nTo see the full end-to-end example used to create this demo, check out [my GitHub repo](https://github.com/tbarlow12/sls-az-func-rest-api). I structured [each commit](https://github.com/tbarlow12/sls-az-func-rest-api/commits/master) to follow the steps described in this post. Any steps named `Step X.X` are steps that involve no code or configuration changes (and thus not tracked by source control), but actions that could/should be taken at that point in the process. This is done to preserve the \"commit-per-step\" structure of the example repo.\r\n\r\nThis post will only cover the basics of creating and deploying a REST API with Azure Functions, which includes [step 1](https://github.com/tbarlow12/sls-az-func-rest-api/commit/6cd5deebf34645f1ebc829d590e0b169e6c23e29) and [step 2](https://github.com/tbarlow12/sls-az-func-rest-api/commit/5ac83c915e7e78ecfe8e30e03c8425d09c1de936) from the example repo. Stay tuned for posts on the additional steps in the future.\r\n\r\nI will make the assumption that you have the Serverless Framework installed globally. If you do not (or have not updated in a while), run:\r\n\r\n```\r\nnpm i serverless -g\r\n```\r\n\r\nAlso, the `serverless` CLI can be referenced by either `serverless` or `sls`. I will use `sls` in this post just because it's shorter, but `serverless` would work just the same.\r\n\r\n## Step 1: Create your local Azure Function project\r\n\r\nLet's begin by creating our Azure Function project with a template from serverless.\r\n\r\n```\r\nsls create -t azure-nodejs -p sls-az-func-rest-api\r\n```\r\n\r\nThe resulting project will be in the directory `sls-az-func-rest-api`. `cd` into that directory and run `npm install`. To make sure you have the latest version of the Azure Functions plugin, run:\r\n\r\n```\r\nnpm install serverless-azure-functions --save\r\n```\r\n\r\nItâ€™s important to note that the generated `serverless.yml` file will contain a lot of commented lines, which start with `#`. Those are purely for your benefit in exploring features of the Azure Functions plugin, and can be safely removed.\r\n\r\n## Step 2: Add your own handlers\r\n\r\nFor the sake of this demo, weâ€™re going to create a basic wrapper of the GitHub API for [issues](https://developer.github.com/v3/issues/) and [pull requests](https://developer.github.com/v3/pulls/).\r\n\r\nAs youâ€™ve probably already noticed, the `azure-nodejs` [template](https://github.com/serverless/serverless/tree/master/lib/plugins/create/templates/azure-nodejs) comes preloaded with two functions: `hello` and `goodbye`. Letâ€™s remove those before we start adding our own code. To do this, remove both the `hello.js` and `goodbye.js` files. Also, remove their configuration definitions from `serverless.yml`.\r\n\r\nRight now your file structure should look something like:\r\n\r\n```\r\nsls-az-func-rest-api\r\n|-- host.json\r\n|-- package.json\r\n|-- README.md\r\n|-- serverless.yml\r\n```\r\n\r\nand your `serverless.yml` should look like (not including any comments):\r\n\r\n```yaml\r\nservice: sls-az-func-rest-api \r\n \r\nprovider:\r\n  name: azure\r\n  region: West US 2\r\n  runtime: nodejs10.x\r\n \r\nplugins:\r\n  - serverless-azure-functions\r\n \r\npackage:\r\n  exclude:\r\n    - local.settings.json\r\n    - .vscode/**\r\n \r\nfunctions:\r\n```\r\n\r\n### Add Code\r\n\r\nLetâ€™s add in our own code. Weâ€™ll start by creating the directory `src/handlers`. This, perhaps to your great surprise, will be where our handlers will live. Inside that directory, we will put our two handlers: [issues.js](https://github.com/tbarlow12/sls-az-func-rest-api/blob/master/src/handlers/issues.js) and [pulls.js](https://github.com/tbarlow12/sls-az-func-rest-api/blob/master/src/handlers/pulls.js).\r\n\r\n```javascript\r\n// src/handlers/issues.js\r\n\r\nconst utils = require(\"../utils\");\r\nconst axios = require(\"axios\");\r\n\r\nmodule.exports.handler = async (context, req) => {\r\n  context.log(\"Issue Handler hit\");\r\n\r\n  const owner = utils.getQueryOrBodyParam(req, \"owner\");\r\n  const repo = utils.getQueryOrBodyParam(req, \"repo\");\r\n\r\n  if (owner && repo) {\r\n    const response = await axios({\r\n      url: `https://api.github.com/repos/${owner}/${repo}/issues`,\r\n      method: \"get\"\r\n    });\r\n    context.res = {\r\n      status: 200,\r\n      body: response.data\r\n    };\r\n  } else {\r\n    context.res = {\r\n      status: 400,\r\n      body: \"Please pass the name of an owner and a repo in the request\"\r\n    };\r\n  }\r\n};\r\n```\r\n```javascript\r\n// src/handlers/pulls.js\r\n\r\nconst utils = require(\"../utils\");\r\nconst axios = require(\"axios\");\r\n\r\nmodule.exports.handler = async (context, req) => {\r\n  context.log(\"Pull Request Handler hit\");\r\n\r\n  const owner = utils.getQueryOrBodyParam(req, \"owner\");\r\n  const repo = utils.getQueryOrBodyParam(req, \"repo\");\r\n  \r\n  if (owner && repo) {\r\n    const response = await axios({\r\n      url: `https://api.github.com/repos/${owner}/${repo}/pulls`,\r\n      method: \"get\"\r\n    });\r\n    context.res = {\r\n      status: 200,\r\n      body: response.data\r\n    };\r\n  } else {\r\n    context.res = {\r\n      status: 400,\r\n      body: \"Please pass the name of an owner and a repo in the request\"\r\n    };\r\n  }\r\n};\r\n```\r\n\r\nJust for fun, weâ€™ll also add a [utils.js](https://github.com/tbarlow12/sls-az-func-rest-api/blob/master/src/utils.js) file for shared utility functions across handlers, and weâ€™ll put that just inside the `src` directory.\r\n\r\n```javascript\r\n// src/utils.js\r\n\r\n/** Gets the param from either the query string\r\n * or body of request\r\n */\r\nmodule.exports.getQueryOrBodyParam = (req, param) => {\r\n  const { query, body } = req;\r\n  if (query && query[param]) {\r\n    return query[param];\r\n  }\r\n  if (body && body[param]) {\r\n    return body[param];\r\n  }\r\n};\r\n```\r\n\r\nYouâ€™ll also note that the handlers are using a popular NPM package for HTTP requests, `axios`. Run `npm install axios --save` in your service root directory.\r\n\r\n### Current Folder structure\r\n\r\n```\r\nsls-az-func-rest-api\r\n|-- src\r\n    |-- handlers\r\n        |-- issues.js\r\n        |-- pulls.js\r\n    |-- utils.js\r\n|-- host.json\r\n|-- package.json\r\n|-- README.md\r\n|-- serverless.yml\r\n```\r\n\r\nNow we need to add our new handlers to the serverless configuration, which will now look like:\r\n\r\n```yaml\r\nservice: sls-az-func-rest-api \r\n \r\nprovider:\r\n  name: azure\r\n  location: West US 2\r\n \r\nplugins:\r\n  - serverless-azure-functions\r\n \r\npackage:\r\n  exclude:\r\n    - local.settings.json\r\n    - .vscode/**\r\n \r\nfunctions:\r\n  issues:\r\n    handler: src/handlers/issues.handler\r\n    events:\r\n      - http: true\r\n        x-azure-settings:\r\n          authLevel: anonymous\r\n  pulls:\r\n    handler: src/handlers/pulls.handler\r\n    events:\r\n      - http: true\r\n        x-azure-settings:\r\n          authLevel: anonymous\r\n```\r\n \r\n## Step 2.1: Test your API Locally\r\n\r\nRun the following command in your project directory to test your local service.\r\n\r\n```bash\r\nsls offline\r\n```\r\n\r\nThis will generate a directory for each of your functions with the file `function.json` in each of those directories. This file contains metadata for the â€œbindingsâ€ of the Azure function, and will be cleaned up when you stop the process. You shouldnâ€™t try to change the bindings files yourself, as they will be cleaned up and regenerated from `serverless.yml`. If you make changes to your `serverless.yml` file, youâ€™ll need to exit the process and restart. Changes to code, however, will trigger a hot reload and wonâ€™t require a restart.\r\n\r\nHere is what you can expect as output when you run `sls offline`:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/offline.png)\r\n\r\nWhen you see the â€œHttp Functionsâ€ in the log, you are good to invoke your local service.\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/urls.png)\r\n\r\nOne easy way to test your functions is to start up the offline process in one terminal, and then in another terminal, run:\r\n\r\n```bash\r\nsls invoke local -f {functionName} -p {fileContainingTestData.json}\r\n```\r\n\r\nLetâ€™s create a file with some sample data at the root of our project, and weâ€™ll just call it `data.json`:\r\n\r\n```json\r\n{\r\n  \"owner\": \"serverless\",\r\n  \"repo\": \"serverless-azure-functions\"\r\n}\r\n```\r\n\r\nLuckily, `owner` and `repo` are the same parameters expected by both the `issues` and `pulls` handlers, so we can use this file to test both.\r\n\r\nWeâ€™ll keep our `offline` process running in one terminal. Iâ€™ll open up another (pro tip: use the â€œSplit Terminalâ€ in the VS Code integrated terminal), and run:\r\n\r\n```bash\r\nsls invoke local -f pulls -p data.json\r\n```\r\n\r\nHereâ€™s my output:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/invokeLocal.png)\r\n\r\nYou can see that it made a `GET` request to the locally hosted API and added the info from `data.json` as query parameters. There are no restrictions on HTTP methods, you would just need to specify in the CLI if itâ€™s not a `GET`. (Example: `sls invoke local -f pulls -p data.json -m POST`)\r\n\r\nYou could also run a simple `curl` command that would accomplish the same thing:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/curl.png)\r\n\r\nAnd here is the output in the terminal running the API. You can see our `console.log` statement from the handler output here:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/handlerLog.png)\r\n\r\nWhen Iâ€™m done running the service locally, Iâ€™ll hit `Ctrl/Cmd + C` in the API terminal to stop the process. You can see that it cleans up those metadata files we discussed earlier:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/cleanup.png)\r\n\r\n## Step 2.2: Deploy\r\n\r\n### Authentication\r\n\r\nThatâ€™s all the configuration we need, so weâ€™re ready to deploy this Function App. In order to deploy, weâ€™ll need to authenticate with Azure. There are two options for authentication: interactive login and a service principal (which, if you are unfamiliar, is essentially a service account). \r\n\r\nAt first, when you run a command that requires authentication, the Interactive Login will open up a webpage for you to enter a code. Youâ€™ll only need to do this once. The authentication results are cached to your local machine. \r\n\r\nIf you have a service principal, youâ€™ll set the appropriate environment variables on your machine, and the plugin will skip the interactive login process. Unfortunately, if youâ€™re using a free trial account, your only option is a service principal. The process for creating one and setting up your environment variables is detailed in the [Azure plugin README](https://github.com/serverless/serverless-azure-functions#creating-a-service-principal).\r\n\r\n### Deploy Command\r\n\r\nWith configuration and authentication in place, letâ€™s ship this thing. From the root of your project directory, run:\r\n\r\n`sls deploy`\r\n\r\nand watch the magic happen. Your app will be packaged up into a `.zip` file, which will be located in the `.serverless` directory at the root of your project. From there, an Azure resource group will be created for your application, containing things like your storage account, Function App, and more. After the resource group is created, the zipped code will be deployed to your newly created function app and the URLs for your functions will be logged to the console.\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/deploy+(1).png)\r\n\r\n## Step 2.3 Invoke Deployed Function\r\n\r\nWe can invoke a deployed function in the same way we invoked our local function, just without the `local` command:\r\n\r\n```\r\nsls invoke -f pulls -p data.json\r\n```\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure+plugin+update/invoke.png)\r\n\r\n## (Optional) Step 2.4: Cleanup\r\n\r\nIf you have been following this tutorial and would like to clean up the resources you deployed, you can simply run:\r\n\r\n```\r\nsls remove\r\n```\r\n\r\nBE CAREFUL when running this command. This will delete your entire resource group.\r\n\r\n## Additional Steps\r\n\r\nStay tuned for future posts walking you through other steps of setting up your service, including adding [API Management](https://azure.microsoft.com/en-us/services/api-management/) configuration, quality gates like linting and unit tests, adding Webpack support, CI/CD and more.\r\n\r\nAlso, if you're going to be at ServerlessConf 2019 in NYC, the Microsoft team is putting on a [Azure Serverless Hands-on Workshop](http://aka.ms/nycworkshop) on October 7th from 8:30 am to 5:00 pm.\r\n\r\n## Contributing\r\n\r\nWeâ€™re eager to get your feedback on the `serverless-azure-functions` plugin. Please [log issues on the GitHub repo with any bug reports or feature requests](https://github.com/serverless/serverless-azure-functions/issues/new/choose). Or better yet, fork the repo and open up a [pull request](https://github.com/serverless/serverless-azure-functions/pulls)! \r\n","data":{"title":"How to Create a REST API with Azure Functions and the Serverless Framework - Part 1","date":"2019-09-17T00:00:00.000Z","path":"serverless-part1"},"isEmpty":false,"excerpt":""},"serverless-part2":{"content":"\r\n#### Overview\r\n\r\n(_See the [original post here](https://serverless.com/blog/serverless-azure-functions-v2)._)\r\n\r\nNow that you've created and deployed a basic API from [Part 1](https://serverless.com/blog/serverless-azure-functions-v1), let's take a few more steps towards making that API more resilient and secure. This post will still be based on the [example repo](https://github.com/tbarlow12/sls-az-func-rest-api), and will follow the same \"commit-per-step\" format as [Part 1](https://serverless.com/blog/serverless-azure-functions-v1), which contains Steps 1 and 2.\r\n\r\nTo pick up where we left off in the example repo (after having completed Step 2), run:\r\n\r\n```bash\r\n# Assumes you've already forked the repo\r\n$ git clone https://github.com/<your-github-name>/sls-az-func-rest-api && git checkout cf46d1d\r\n```\r\n\r\n#### Step 3: Add unit testing and linting - (commit [465ecfe](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f))\r\n\r\nBecause this isn't a blog post on unit tests, linting or quality gates in general, I'll just share the tools that I'm using and the quality gates that I added to the repository. Feel free to use them as stubs for your own future tests or lint rules.\r\n\r\nFor unit tests, I'm using the [Jest](https://jestjs.io/) test runner from Facebook. I've used it for several projects in the past and have never had any issues. Jest tests typically sit alongside the file they are testing, and end with `.test.js`. This is configurable within [`jest.config.js`](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f#diff-2d0cd5d10b9604941c38c6aac608178a), which is found at the root of the project.\r\n\r\nBecause my code makes REST calls via `axios`, I'm using the `axios-mock-adapter` to mock the request & response. The tests that I wrote ([issues.test.js](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f#diff-fb5daf13ab24c55eef4f041fc89c5025) and [pulls.test.js](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f#diff-29c6cbdb5c35cdd4da7f67589ae7121a)) run some simple checks to make sure the correct URLs are hit and return the expected responses.\r\n\r\nFor linting, I'm using [ESLint](https://eslint.org) with a very basic configuration, found in [`.eslintrc.json`](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f#diff-df39304d828831c44a2b9f38cd45289c). To run a lint check, you can run:\r\n\r\n```bash\r\n$ npm run lint\r\n```\r\n\r\nMany errors can be fixed automatically with:\r\n\r\n```bash\r\n$ npm run lint:fix\r\n```\r\n\r\nRun your tests with:\r\n\r\n```bash\r\n$ npm test\r\n```\r\n\r\nFor more details, take a look at the [commit in the example repo](https://github.com/tbarlow12/sls-az-func-rest-api/commit/465ecfe04bda8d4d5ac7c9c5ce31557a8993408f) or check out the commit locally\r\n\r\n```bash\r\n$ git checkout 465ecfe\r\n```\r\n\r\n#### Step 4: Add basic API Management Configuration - (commit [c593308](https://github.com/tbarlow12/sls-az-func-rest-api/commit/c593308efc5a60e2701ec97122564592072080e2))\r\n\r\nThis was one of the first features we implemented into the `v1` of the `serverless-azure-functions` plugin. because most Azure Function Apps are REST APIs, and it's hard to have a real-world API in Azure without [API Management](https://azure.microsoft.com/en-us/services/api-management/).\r\n\r\nIf you have no special requirements for API Management, the plugin will actually generate the default configuration for you if you just include:\r\n\r\n```yaml\r\n...\r\nprovider:\r\n    ...\r\n    apim: true\r\n```\r\n\r\nThat's exactly what I did for [Step 4](https://github.com/tbarlow12/sls-az-func-rest-api/commit/c593308efc5a60e2701ec97122564592072080e2). Also, because we want API Management to be the only entry point for our API endpoints, I also changed each function's `authLevel` to `function`. This requires a function-specific API key for authentication. You can see in the screenshot what happens in the first command, when I try to `curl` the original function URL. I get a `401` response code. But when I hit the URL provided by API Management, I get the response I expect:\r\n\r\n![alt text](https://s3-us-west-2.amazonaws.com/assets.blog.serverless.com/azure-functions-part2/apim_curl.jpg)\r\n\r\nFor more details on `authLevel`, check out the [trigger configuration docs](https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-http-webhook#trigger---configuration).\r\n\r\n\r\n###### Consumption SKU\r\n\r\nOne important thing to note is that the API Management configuration will default to the `consumption` SKU, which [recently went GA](https://azure.microsoft.com/en-ca/updates/azure-api-management-consumption-tier-is-now-generally-available/). For now, the only regions where `Consumption` API Management is allowed are:\r\n\r\n- North Central US\r\n- West US\r\n- West Europe\r\n- North Europe\r\n- Southeast Asia\r\n- Australia East\r\n\r\nIf you are deploying to a region outside of that list, you will need to specify a different SKU (`Developer`, `Basic`, `Standard` or `Premium`) within the `apim` configuration, which will be demonstrated in the next section.\r\n\r\n###### Deploy your updates:\r\n\r\n```bash\r\n$ sls deploy\r\n```\r\n\r\n#### Step 5: Add more advanced API Management Configuration - (commit [38413a0](https://github.com/tbarlow12/sls-az-func-rest-api/commit/38413a03100a65c423dc18ab47754471a4c6f245))\r\n\r\nIf you need a few more knobs to turn when configuring your API Management instance, you can provide a more verbose configuration. Here is the verbose config I added to the sample repo (the `...` means the rest of the config for that section stayed the same):\r\n\r\n```yaml\r\nservice: sls-az-func-rest-api\r\n\r\nprovider:\r\n  ...\r\n  apim:\r\n    apis:\r\n      - name: github-api\r\n        # Require an API Key if true\r\n        subscriptionRequired: false\r\n        displayName: Github API\r\n        description: The GitHub API\r\n        protocols:\r\n          - https\r\n        # Defaults to /api\r\n        path: github\r\n        # Azure resource tags\r\n        tags:\r\n          - apimTag1\r\n          - apimTag2\r\n        authorization: none\r\n    backends:\r\n      - name: github-backend\r\n        url: api/github\r\n    cors:\r\n      allowCredentials: false\r\n      allowedOrigins:\r\n        - \"*\"\r\n      allowedMethods:\r\n        - GET\r\n        - POST\r\n        - PUT\r\n        - DELETE\r\n        - PATCH\r\n      allowedHeaders:\r\n        - \"*\"\r\n      exposeHeaders:\r\n        - \"*\"\r\n...\r\n\r\nfunctions:\r\n  issues:\r\n    ...\r\n    apim:\r\n      api: github-api\r\n      backend: github-backend\r\n      operations:\r\n        - method: get\r\n          urlTemplate: /issues\r\n          displayName: GetIssues\r\n  pulls:\r\n    ...\r\n    apim:\r\n      api: github-api\r\n      backend: github-backend\r\n      operations:\r\n        - method: get\r\n          urlTemplate: /pulls\r\n          displayName: GetPullRequests\r\n```\r\n\r\nIf you did not want the `Consumption` SKU of API Management, you would need to have a verbose configuration and specify the `sku` as:\r\n\r\n```yaml\r\nprovider:\r\n  ...\r\n  apim:\r\n    ...\r\n    sku:\r\n      name: {Consumption|Developer|Basic|Standard|Premium}\r\n```\r\n\r\nThe example just uses the default and deploys to region(s) where Consumption API Management is currently available.\r\n\r\n###### Deploy your updates:\r\n\r\n```bash\r\n$ sls deploy\r\n```\r\n\r\n#### (Optional) Step 5.1: Revert back to basic API Management configuration - (commit [4c5803f](https://github.com/tbarlow12/sls-az-func-rest-api/commit/4c5803f1e5adf21befbeac8e91cac4552b4f9c1c))\r\n\r\nTo make the demo simple and easy to follow, I'm going to revert my `apim` configuration back to the defaults:\r\n\r\n```yaml\r\napim: true\r\n```\r\n\r\nYou might be able to do the same, depending on your requirements.\r\n\r\n#### Step 6: Add Webpack configuration - (commit [1aefac7](https://github.com/tbarlow12/sls-az-func-rest-api/commit/1aefac7e5ed99db009632724c6a70c9cb3d29bf8))\r\n\r\n[Webpack](https://webpack.js.org/) dramatically reduces the packaging time as well as the size of your deployed package. After making these changes, your packaged Function App will be optimized with Webpack (You can run `sls package` to package it up or just run `sls deploy` which will include packaging as part of the lifecycle).\r\n\r\nJust as an example, even for this very small application, my package size went from **324 KB** to **28 KB**. \r\n\r\nTo accomplish this, we'll use another awesome Serverless plugin, [`serverless-webpack`](https://github.com/serverless-heaven/serverless-webpack) to make Webpacking our Azure Function app really easy.\r\n\r\nFirst thing you'll want to do, assuming you're working through this tutorial in your own git repository, is add the generated Webpack folder to your `.gitignore`\r\n\r\n```yaml\r\n# .gitignore\r\n...\r\n# Webpack artifacts\r\n.webpack/\r\n```\r\n\r\nNext, we'll need to install 3 packages from npm:\r\n\r\n```bash\r\n$ npm i serverless-webpack webpack webpack-cli --save-dev\r\n```\r\n\r\nThen we'll add the plugin to our `serverless.yml`:\r\n\r\n```yaml\r\nplugins:\r\n  - serverless-azure-functions\r\n  - serverless-webpack\r\n```\r\n\r\nAnd then copy this exact code into `webpack.config.js` in the root of your service directory:\r\n\r\n```javascript\r\nconst path = require(\"path\");\r\nconst slsw = require(\"serverless-webpack\");\r\n\r\nmodule.exports = {\r\n  entry: slsw.lib.entries,\r\n  target: \"node\",\r\n  output: {\r\n    libraryTarget: \"commonjs2\",\r\n    library: \"index\",\r\n    path: path.resolve(__dirname, \".webpack\"),\r\n    filename: \"[name].js\"\r\n  },\r\n  plugins: [],\r\n};\r\n```\r\nAnd just like that, your deployed Azure Function apps will be webpacked and ready to go.\r\n\r\n![alt text](https://media.giphy.com/media/zcCGBRQshGdt6/giphy.gif)\r\n\r\n#### Step 7: Enable Serverless CLI configuration - (commit [4cb42fd](https://github.com/tbarlow12/sls-az-func-rest-api/commit/4cb42fdf17d7793a3ac9660bb43f28e8fe2d46d5))\r\n\r\nIf you're running a real-life production service, you will most likely be deploying to multiple regions and multiple stages. Maybe merges to your `dev` branch will trigger deployments to your `dev` environment, `master` into `prod`, etc. I'll show you an example of that in Step 8. To accomplish CLI-level configurability, we need to make a few changes `serverless.yml`.\r\n\r\n```yaml\r\nprovider:\r\n  region: ${opt:region, 'West US'}\r\n  stage: ${opt:stage, 'dev'}\r\n  prefix: ${opt:prefix, 'demo'}\r\n```\r\n\r\nAs you might have guessed, the values `West US`, `dev` and `demo` are my default values. If I wanted to deploy my service to `North Central US` and `West Europe`, but keep everything else the same, I would run:\r\n\r\n```bash\r\n$ sls deploy --region \"North Central US\"\r\n$ sls deploy --region \"West Europe\"\r\n```\r\n\r\nWe could do similar operations with `--prefix` and `--stage`. Now let's create a pipeline that actually does this.\r\n\r\n#### Step 8: Add CI/CD (with Azure DevOps) - (commit [a8fabf6](https://github.com/tbarlow12/sls-az-func-rest-api/commit/a8fabf6faa30f7ceab7c18395a5c69c21abd4640))\r\n\r\nFor the CI/CD on my sample repo, I'm using [Azure DevOps](), but it would work the same on any other service you want to use. If you want to use Azure DevOps for an open-source project, [here are a few steps to get started](https://docs.microsoft.com/en-us/azure/devops/organizations/public/about-public-projects?view=azure-devops#get-started-with-a-public-project)\r\n\r\nNo matter the CI/CD environment, here is what we are looking to accomplish:\r\n\r\n1. Install dependencies\r\n2. Validate the changes (run quality gates)\r\n3. Deploy the service\r\n\r\nThese steps can all be accomplished in just a few CLI commands. At bare minimum, we'll want to run something like:\r\n\r\n```bash\r\n# Clean install\r\nnpm ci\r\n# Runs tests and linting\r\nnpm test\r\n# Serverless not contained within dev dependencies to avoid conflicts\r\n# because most users have it installed globally on their dev machine\r\nnpm i serverless -g\r\n# Deploy service\r\nsls deploy\r\n```\r\n\r\nThere are a lot more bells and whistles we could add, but that's essentially what it boils down to. Of course, we'll need authentication in whatever system we're deploying from, and that's where the [service principal](https://github.com/serverless/serverless-azure-functions#creating-a-service-principal) will come in. I'll show you how to use the service principal in the `deploy.yml` pipeline below.\r\n\r\nFor my pipelines, I'm actually going to split up my CI and CD into `unit-tests.yml` and `deploy.yml`. Unit tests will be run on PRs into `master` or `dev` (this is assuming there are branch policies in place to prevent devs from pushing straight to either branch). Deployment will be run on commits (merges) to `master`.\r\n\r\n##### Unit Tests\r\n```yaml\r\n# pipelines/unit-tests.yml\r\n\r\n# Only run on Pull Requests into `master` or `dev`\r\npr:\r\n  branches:\r\n    include:\r\n    - master\r\n    - dev\r\n\r\n# Run pipeline on node 8 and 10 on Linux, Mac and Windows \r\nstrategy:\r\n  matrix:\r\n    Linux_Node8:\r\n      imageName: 'ubuntu-16.04'\r\n      node_version: 8.x\r\n    Linux_Node10:\r\n      imageName: 'ubuntu-16.04'\r\n      node_version: 10.x\r\n    Mac_Node8:\r\n      imageName: 'macos-10.14'\r\n      node_version: 8.x\r\n    Mac_Node10:\r\n      imageName: 'macos-10.14'\r\n      node_version: 10.x\r\n    Windows_Node8:\r\n      imageName: 'win1803'\r\n      node_version: 8.x\r\n    Windows_Node10:\r\n      imageName: 'win1803'\r\n      node_version: 10.x\r\n\r\n# https://docs.microsoft.com/en-us/azure/devops/pipelines/agents/hosted?view=azure-devops#use-a-microsoft-hosted-agent\r\npool:\r\n  vmImage: $(imageName)\r\n\r\nsteps:\r\n- task: NodeTool@0\r\n  inputs:\r\n    versionSpec: $(node_version)\r\n  displayName: 'Install Node.js'\r\n\r\n# Make pipeline fail if tests or linting fail, linting occurs in `pretest` script\r\n- bash: |\r\n    set -euo pipefail\r\n    npm ci\r\n    npm test\r\n  displayName: 'Run tests'\r\n```\r\n\r\n##### Deployment\r\n```yaml\r\n# pipelines/deploy.yml\r\n\r\ntrigger:\r\n  branches:\r\n    include:\r\n    - master\r\n\r\n# https://docs.microsoft.com/en-us/azure/devops/pipelines/library/variable-groups?view=azure-devops&tabs=yaml\r\nvariables:\r\n- group: sls-deploy-creds\r\n\r\njobs:\r\n\r\n- job: \"Deploy_Azure_Function_App\"\r\n  timeoutInMinutes: 30\r\n  cancelTimeoutInMinutes: 1\r\n\r\n  pool:\r\n    vmImage: 'ubuntu-16.04'\r\n\r\n  steps:\r\n  - task: NodeTool@0\r\n    inputs:\r\n      versionSpec: 10.x\r\n    displayName: 'Install Node.js'\r\n\r\n  - bash: |\r\n      npm install -g serverless\r\n    displayName: 'Install Serverless'\r\n    # Deploy service with prefix `gh`, stage `prod` and to region `West Europe`\r\n  - bash: |\r\n      npm ci\r\n      sls deploy --prefix gh --stage prod --region \"West Europe\"\r\n    env:\r\n      # Azure Service Principal. Secrets need to be mapped here\r\n      # USE THIS EXACT TEXT, DON'T COPY/PASTE YOUR CREDENTIALS HERE.\r\n      # Azure DevOps will use the variables within\r\n      # the variable group `sls-deploy-creds` to replace all the $() values\r\n      AZURE_SUBSCRIPTION_ID: $(AZURE_SUBSCRIPTION_ID)\r\n      AZURE_TENANT_ID: $(AZURE_TENANT_ID)\r\n      AZURE_CLIENT_ID: $(AZURE_CLIENT_ID)\r\n      AZURE_CLIENT_SECRET: $(AZURE_CLIENT_SECRET)\r\n    displayName: 'Deploy Azure Function App' \r\n```\r\n\r\nNotice [this line](https://github.com/tbarlow12/sls-az-func-rest-api/blob/master/pipelines/deploy.yml#L30) in the deployment pipeline that leverages our setup from Step 7. You might have multiple pipelines for the different stages, you might dynamically infer these values from the branch name or you might just provide the values as environment variables. The point of the setup in Step 7 was to provide you the flexibility to deploy your service to wherever you see fit at the time, without needing to change your `serverless.yml` file.\r\n\r\n#### Concluding Thoughts\r\n\r\nA big part of our reason for investing time and effort into the `serverless-azure-functions` plugin was so that developers could easily deploy Azure Functions to solve more real-world, business-level scenarios. We hope that as you use the tool and discover areas for improvement that you'll [file issues on the repo](https://github.com/serverless/serverless-azure-functions/issues/new/choose) or even open up a [pull request](https://github.com/serverless/serverless-azure-functions/pulls).\r\n","data":{"title":"How to Create a REST API with Azure Functions and the Serverless Framework - Part 2","date":"2019-10-1","path":"serverless-part2"},"isEmpty":false,"excerpt":""},"thedifference":{"content":"\r\nWe've all been there. Talking with old friends or new acquaintances about your career as a software engineer, and those magical words pop into the conversation: \"_Bro... I have an idea for this app..._\" Much to the surprise and chagrin of many aspiring Zuckerbergs, success comes from a heck of a lot more than just a good idea.\r\n\r\nMy purpose here is not to disparage any idea. I think some of the best ideas in the world are those that sound insane from the start. My purpose is to reflect on one of my own experiences and use it to identify some of the things that separate the successful ideas from... well... the others.\r\n\r\n![alt text](/images/kramer.gif)\r\n\r\nThis experience I'm talking about was a time that I was able to witness the end-to-end process of an idea becoming a success. It started similarly to the other experiences I referenced in the beginning of this post - a friend came to me with an idea, and an ambitious one at that. \r\n\r\n_(cue [flashback music from \"Arthur\"](https://www.youtube.com/watch?v=KSm377MSv7Y))_\r\n\r\n## The Idea\r\n\r\nIt was April of 2016. I was just about to finish one of the most difficult semesters I'd had to date. I was trying to understand Turing machines, Database relational algebra, and every sorting algorithm ever made since the caveman stacked the biggest rocks at the bottom. For those of you who haven't experienced this, just imagine you're drowning... You cry out for help, hoping someone is close enough hear you... But the only response you get is from your terminal: `Segmentation fault`. \r\n\r\nIn the heat of the confusion, rage and tears, this friend approached me with an idea to create the biggest hackathon in the state of Utah. He wanted it to happen in the upcoming semester (in 5 months) and that he wanted me to help him do it. \r\n\r\nAt the time, I thought, _\"**How** could we ever do that? **Where** would we even find the time? **Why** put more on our plate than we already have?\"_ \r\n\r\nIt's safe to say I had my doubts. But after thinking about it for a few days, I thought it sounded kind of fun and that I'd join him. He recruited a few of our other classmates that were either crazy enough to say \"yes\" or too scared to tell him \"no.\" We had ourselves a team.\r\n\r\n## The Team\r\n\r\nOur first official meeting was a few weeks later, and was actually a video conference, as several of us were working at internships in different states. We jumped on the call and realized that none of us really knew any of the others, except for this friend that had pulled us into this. We weren't anti-social, but we each had our own spheres and didn't often venture outside them. \r\n\r\nI thought to myself, \"This is quite possibly the most random possible sampling of students in the University of Utah Computer Science program...\" Looking back, I realize that these were some of the first indications of differences between this idea and many of the others I'd heard:\r\n- **Assemble a *diverse* team** - Seriously, we could have been a poster for a University marketing campaign. We had just about every demographic covered. But diversity became much more than a box to check. A team with different backgrounds, perspectives, connections and opinions would become crucial to the success of the hackathon.\r\n- **Find the *right* people** - Rather than just pick his friends, he went out and picked people that would be right for each job and helped them catch the vision. I'm not saying the people on the team weren't his friends, but I think it's easy to fall into the trap of just defaulting to your circle of closest friends, even if they're not right for the job.\r\n\r\n## The Sacrifice\r\n\r\nI'll never forget the day we went to ASUU (Associated Students of the University of Utah), our school's student government, to ask for their financial sponsorship of our event. We came on a day where other clubs and student groups were being grilled by the student-legislators over requests in the $50-$100 range. We were coming in asking for $3,000. \r\n\r\nBut not only did we believe this event would be fun and impactful for each of the participants personally, we strongly felt it would benefit the University and the community in the long run. \r\n\r\nBig hackathons were starting to become one of the ways students from other schools in and out of the state were exposed to universities, and many of them would eventually plan on going to graduate school. We felt that if we could just give these students a chance to see what it's like to be at the University of Utah, more would consider the U for future educational opportunities.\r\n\r\nThese hackathons are also *major* opportunities for companies to recruit potential candidates (hence the corporate sponsors that shelled out $$$ for a booth at the event and their logo on our materials and website). If we could host a large event where students from around the state and country could get face time with local companies, it was possible that many of those participants would receive job offers, work in the area, and boost the local economy. \r\n\r\nSo, yes, $3000 from the University seemed like a lot up front, but we felt like the investment would pay for itself many times over.\r\n\r\nWhen we got up to pitch the idea to this group of our peers, many of them caught the vision, but there were also some that just couldn't get over the amount of money we were asking for. \r\n\r\nOne of them posed the question, *\"What will you do if you don't get the money?\"*\r\n\r\nMy friend answered without hesitation, *\"It will come out of my own pocket. I believe in this cause and it's something I'm willing to pay for if we don't get the funding.\"* I was as surprised as each of them. Which brings me to the next difference:\r\n\r\n- **Be willing to make *real* sacrifices** - It's not like he was made of money, either. He worked long hours at the hospital, but he also paid for his own education. In my mind, he was offering more than what seemed \"reasonable\" to the rest of us. A sacrifice is more than just giving up something. It's something that actually *hurts* that you do anyway because of your belief in a cause. He believed in the cause and was willing to hurt for it.\r\n\r\n## The Work\r\n\r\nWe each went about our duties, as busy as we were. We did our best to attend weekly meetings on our progress, where we'd identify any blockers and discuss solutions. It was *a lot* of work for everyone involved.\r\n\r\nWe had team members over categories like Marketing, Social Media, Hospitality, Sponsorship, and others. I was the Director of Technology, which meant I would be responsible for the hardware lab, coordinating the schedule and tasks for all of our volunteers and oversee any technical mentoring that was needed.\r\n\r\nHowever, when it came time for the event, even though we still each owned our piece of the pie, we all pitched in to help each other and make it happen. This was something I'd read from the great [John Wooden](/woodeinisms/), but that was reinforced in working with this team:\r\n \r\n- **\"It is amazing how much can be accomplished if no one cares who gets the credit.â€** - No one was above helping anyone else with any activity. We all did our best to operate as a team rather than just make sure our own responsibilities were covered. We all did everything from setting up chairs to serving snacks to running games to renting hardware to schmoozing sponsors and everything else in between.\r\n\r\n\r\n## The Iteration\r\n\r\nAfter a successful hackathon with very few hiccups, I thought we had earned a well-deserved rest. I was sure there were a few complaints here or there from participants and sponsors alike, but hey, in an event with 180+ participants and ~15 sponsors several employees each, there was bound to be at least _some_ whining. \r\n\r\nIt wasn't that I didn't care, I just thought we had a little bit of time before we needed to worry about it again. My friend felt differently. After the feedback survey came back, he personally spoke with each sponsor, apologizing if necessary for any inconvenience and pledging to make the changes for the next year. I watched as those items of feedback tormented him over the next year, giving each one a great deal of time and attention, making sure no one would be able to raise that complaint again.\r\n\r\nIn other words, he made this idea different by: \r\n\r\n- **Not only accepting, but *seeking* honest feedback** - It wasn't enough to pat himself on the back for a job well done and rest up. He *needed* to address the issues to make the next event  *perfect*. He didn't want to waste any time in pursuit of that goal.\r\n\r\n## The Rest of the Story\r\n\r\nWe did address those issues and prepare for the next year's Hackathon. Our team locked down a title sponsor that gave us over $12,000 on top of many of our sponsorships from the previous year. Because of the event's success, they actually signed a contract to remain the title sponsor over the next 3 years, increasing the donation by 20% each year. \r\n\r\nWe had over 260 participants that created some amazing products, one of which was a robot that used Tensorflow to learn a participants preferences in girls and used a stylus to swipe left or right on his Tinder application.\r\n\r\nMy friend, [Johnny Le](https://johnnyle.me/), went on to win an [award](http://dailyutahchronicle.com/2018/05/21/ideas-are-only-worth-what-you-make-of-them-u-student-starts-utahs-largest-hackathon/) given to one student each year for student leadership that resulted in a $2,000 cash award and a $10,000 donation to the student organization of his choice (guess which organization he gave it to). \r\n\r\nI sincerely hope that none of what I have said comes across as hyperbole or sugar-coated. Like all teams and projects, we had our issues and problems. Even Johnny, as great a guy as he is, makes mistakes. I'm also not saying that these are the only things one needs to do in order to be successful at any given task. I don't pretend to know the secret sauce to success - I'm still trying to find it myself.\r\n\r\nBut [HackTheU](https://hacktheu.com/) didn't end up in the \"Good Idea Graveyard\" because Johnny and our team treated it differently. I had a front row seat to watch some amazing individuals come together, take collective ownership of a good idea and make it happen through initiative, sacrifice, humility and the unglamorous, frustrating, seemingly never-ending **work**.\r\n\r\n*\"You wonder how they do it <br>\r\nAnd you look to see the knack<br>\r\nYou watch the foot in action,<br>\r\nOr the shoulder or the back,<br>\r\nBut when you spot the answer<br>\r\nWhere the higher glamours lurk<br>\r\nYouâ€™ll find in moving higher<br>\r\nUp the laurel covered spire<br>\r\nThat most of it is practice<br>\r\nAnd the rest of it is work\"<br>\r\n -Grantland Rice*","data":{"title":"The Difference - What Makes an Idea Work","date":"2018-6-15","path":"thedifference"},"isEmpty":false,"excerpt":""},"twenty-questions":{"content":"\r\n## Intro:\r\n\r\n`Name:` Tanner Barlow <br>\r\n`Hometown:` Salt Lake City, Utah <br>\r\n`Alma Mater:` University of Utah <br>\r\n`Team/role:` Commercial Software Engineering <br>\r\n`Funâ€¯question/fact:` My high school mascot is a \"beetdigger\"\r\n \r\n<!-- Microsoft Culture  -->\r\n### 1.\tHow are using Microsoft as a platform from your passions?\r\nI get to write open-source code at work. My team sends me to one conference per year â€“ any conference, any place. I bring my family on most work trips and often extend my stay to explore the different parts of the world we visit. Our management encourages 20% projects (side projects you work on with 20% of your time), which have actually evolved into open source projects that weâ€™ve worked on and contributed to.\r\n\r\n### 2.\tWhat is your favorite building on campus and why?\r\n\r\nThe Mixer â€“ basically any kind of food you could want and a basketball court right outside. \r\n\r\n### 3.\tWhat's the most challenging part of your job?\r\n\r\nConstant change and needing to learn radically different tools & technologies in a short period of time \r\n \r\n<!-- Candidate Value Prop (Commonly asked questions & helpful student tips!) -->\r\n\r\n### 4. Myth or Not: Do you have to be an intern to secure a full-time role? \r\n\r\nMYTH. There are SO many people on my team that never interned anywhere. I was an intern, and loved my experience, but wasnâ€™t a requirement for me to get a job.\r\n\r\n### 5. Interview Prep 101, what is the best resource you would recommend?\r\n\r\nCracking the Coding Interview + LeetCode + Pramp\r\n\r\n### 6.\tWhat's one piece of career/personal advice you'd give you your younger self?\r\n\r\nGet up **early** and get things done before the rest of the day makes demands on your time.\r\n\r\n### 7.\tWhat are ways that you feel empowered to give back to communities and how? \r\n\r\nThe donation matching + volunteer time matching from Microsoft is an amazing benefit that Iâ€™ve used to give back to a local Boy Scout troop Iâ€™ve volunteered with.\r\n\r\n### 8. How did you decide to work at MSFT vs. another company? \r\nMSFT was a lot more family-friendly than most of the companies I was interviewing at, I felt like I could really grow in the team I ended up with, I identified with the vision of â€œempowering othersâ€ from the mission statement\r\n\r\n### 9. Who are your mentors? How did you select/build a relationship with mentor?  \r\n\r\nOne of the managers on a peer team. We met on his first day joining the group. I was still deciding if I should make the switch or not, and he gave me some sage wisdom. Since then, we meet up about every 2-3 weeks for coffee and to chat about whateverâ€™s going on in work and in life. Heâ€™s become an amazing asset and a great friend. Other mentors include my parents, former church leaders and previous managers from my intern days at MSFT.\r\n \r\n<!-- Job Content  -->\r\n### 10. In a quick snapshot, what do you do?\r\n\r\nI write code with Microsoft partners. For 3-5 months, our team pairs up with one of their dev teams and solve whatever interesting problem they are confronting at the time. We do a lot of open-source work (Serverless Framework, VoTT, Cloud Custodian, etc.) and travel around the world to work with these partners.\r\n\r\n### 11. In your mind, what makes a great software engineer?\r\n\r\nGrit, resourcefulness and humility. With those three things, most of the other skills can be acquired over time.\r\n\r\n### 12. What does a typical day look like as a software engineer? How much time do you spend coding? In meetings? etc. What is the size of your team? How does your manager contribute to your overall success? \r\n\r\nMy favorite kinds of days are where I have standup in the morning and the rest of the day to code. Usually, thatâ€™s not a reality. A good day is about 4-5 hours of solid coding time. Our team has 10 engineers, but weâ€™re often split in 2 or 3 squads depending on the projects we have going at the time. My manager is actually right there in the weeds with us â€“ coding alongside us just like other devs on the team. He does a great job of making sure we all have opportunities to stretch and grow.\r\n\r\n### 13. Can you give me more insight into internal mobility? (i.e. how common is it to move teams, how did you do it? Does the power of networking really work?) \r\n\r\nI worked as an intern in the Windows organization for 2 years, and joined this team when I came on full-time. Not a difficult change, I just talked to a manager a few times and did the OneWeek hackathon with him to see if it would be a good fit. It was ðŸ˜Š\r\n\r\n### 14. If you were a former intern, what was your intern project?\r\n\r\n2 projects â€“ automated identification of actionable feedback from Windows users, and another k-means clustering pipeline of windows users based on telemetry received from their machine.\r\n \r\n### 15. If you werenâ€™t a Software Engineer/Program Manager, what would you be?\r\n\r\nAssistant to the Regional Manager of a local paper supply company\r\n\r\n### 16. What is your favorite cafÃ©/restaurant in Seattle?\r\n\r\nChops (Bulgorritoâ€¦ Itâ€™s exactly what it sounds like)\r\n\r\n### 17. What is something that you do outside of work to re-energize?\r\n\r\nCook with my wife, play basketball with my friends or play with my 2-year old son\r\n\r\n### 18. Describe your ideal day in Seattle?\r\n\r\nSummer day on the beach (probably Jetty Island) with my family. Ride the bus into Seattle for a Mariners game, complete with a Seattle dog, garlic fries and post-game fireworks\r\n\r\n### 19. What's your favorite way to eat a potato?\r\n\r\nSliced up and fried into 3-5 inch cubic strands of pure goodness. Otherwise known as fries. Preferably next to a burger. Preferably from Dickâ€™s drive in\r\n\r\n### 20. If you had a superpower, what would it be and why?\r\n\r\nSleep Manipulation (think Mantis from Avengers) so that I could make my kid sleep when heâ€™s supposed to. Either that or flight\r\n\r\n\r\n","data":{"title":"20 Questions - Working at Microsoft","date":"2019-10-15T00:00:00.000Z","path":"twenty-questions"},"isEmpty":false,"excerpt":""},"woodenisms":{"content":"\r\nJohn Wooden was not only one of the greatest coaches of all-time (10 National Championships in 12 seasons at UCLA, 7 of those consecutive) and a great human being, but he had a gift with words. His many one-liners and famous sayings have since become known as \"Woodenisms.\" Here are some of my favorites:\r\n\r\n## Top 10\r\n\r\n1. *\"Be **quick**, but **don't hurry**\"*\r\n\r\n2. *\"Time lost is time lost. Itâ€™s gone forever. Some people tell themselves that they will work twice as hard tomorrow to make up for what they did not do today. People should always do their best. If they work twice as hard tomorrow, then they should have also worked twice as hard today. That would have been their best.\"*\r\n\r\n3. *\"Do not permit what you **cannot** do interfere with what you **can** do.\"*\r\n\r\n4. *\"If I am through learning, I am through.\"*\r\n\r\n5. *\"It is what you learn after you know it all that counts.\"*\r\n\r\n6. *\"Tell the truth. That way you donâ€™t have to remember a story.\"*\r\n\r\n7. *\"Being average means you are as close to the bottom as you are to the top.\"*\r\n\r\n8. *\"Donâ€™t measure yourself by what youâ€™ve accomplished, but rather by what you should have accomplished with your abilities.\"*\r\n\r\n9. *\"If youâ€™re not making mistakes, then youâ€™re not doing anything. Iâ€™m positive that a doer makes mistakes.\"*\r\n\r\n10. *\"You canâ€™t live a perfect day without doing something for someone who will never be able to repay you.\"*\r\n\r\n## Honorable Mentions\r\n- *\"If you keep too busy learning the tricks of the trade, you may never learn the trade.\"*\r\n\r\n- *\"Letâ€™s face it, weâ€™re all imperfect and weâ€™re going to fall short on occasion. But we must learn from failure and that will enable us to avoid repeating our mistakes. Through adversity, we learn, grow stronger, and become better people.\"*\r\n\r\n- *\"Happiness begins where selfishness ends.\"*\r\n\r\n- *\"Never make excuses. Your friends donâ€™t need them and your foes wonâ€™t believe them.\"*\r\n\r\n- *\"You cannot live a perfect day without doing something for another without thought of something in return.\"*\r\n\r\n- *\"We almost have to force or drive ourselves to work hard if we are to reach our potential. If we donâ€™t enjoy what we do, we wonâ€™t be able to push as hard as we need to push for as long as we need to push to achieve our best. However, if we enjoy what we do and if weâ€™re enthusiastic about it, weâ€™ll do it better and come closer to becoming the best we can be.\"*\r\n\r\n- *\"Time spent getting even would be better spent getting ahead.\"*\r\n\r\n- *\"Have character; don't be one.\"*\r\n\r\n- *\"The worst thing about new books is that they keep us from reading the old ones.\"*\r\n\r\n- *\"Never mistake activity for achievement.\"*","data":{"title":"Favorite Woodenisms","date":"2017-10-12T00:00:00.000Z","path":"woodenisms"},"isEmpty":false,"excerpt":""}}